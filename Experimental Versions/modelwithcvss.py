# -*- coding: utf-8 -*-
"""ModelWithCVSS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T8T1ZEwb9aA_ogwG1Ry2aL2GSJy8lkY3
"""

pip install tensorflow pandas openpyxl scikit-learn numpy matplotlib

import pandas as pd              # Data analysis and handling a .csv or .xlsx
import numpy as np              # For math and handling arrays
import tensorflow as tf         # deep learning framework
from tensorflow.keras import regularizers
from sklearn.preprocessing import LabelEncoder  # labelling categorical data (CVE)
from sklearn.model_selection import train_test_split  # for splitting the data
import random                   # needed for masking
from collections import Counter # needed for counting in this model

def load_data(file_path):
    """
    Load data from CSV or Excel file.
    Expected format:
    - ID: identifier for each machine
    - CVE: individual CVE identifier
    - CVSS_Score: CVSS score for the CVE (optional, for future use)
    """
    # we want either a .csv or .xlsx, so we must check it first
    if file_path.endswith('.csv'):
        # use pandas to put the data from the .csv into a df
        df = pd.read_csv(file_path, engine='python')
    elif file_path.endswith('.xlsx'):
        # use pandas to put the data from the .xlsx into a df
        df = pd.read_excel(file_path)
    else:
        # error
        raise ValueError("Upload .csv or .xlsx only")

    # return loaded DataFrame
    return df

from sklearn.preprocessing import StandardScaler

def preprocess_vulnerabilities_with_cvss(df, id_column='ID', cve_column='CVE', cvss_column='CVSS'):
    """
    Process vulnerability data with CVSS scores for model training.

    This function:
    1. Groups CVEs by ID to reconstruct machine vulnerability profiles
    2. Creates lists of CVEs and their corresponding CVSS scores for each machine
    3. Creates bidirectional mappings between CVEs and numerical indices
    4. Processes CVSS scores and creates severity categories

    Args:
        df: pandas DataFrame containing the data
        id_column: name of column containing machine identifiers
        cve_column: name of column containing individual CVE identifiers
        cvss_column: name of column containing CVSS scores

    Returns:
        processed_df: DataFrame with machines as rows and vulnerability lists
        cve_to_idx: dictionary mapping CVE strings to indices
        idx_to_cve: dictionary mapping indices to CVE strings
        cvss_scaler: fitted StandardScaler for CVSS normalization
    """

    # Verify required columns exist
    required_columns = [id_column, cve_column, cvss_column]
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        raise ValueError(f"Missing required columns: {missing_columns}")

    # Clean the data - remove rows with missing CVE, ID, or CVSS values
    df_clean = df.dropna(subset=[id_column, cve_column, cvss_column]).copy()

    # Remove any whitespace from CVE values
    df_clean[cve_column] = df_clean[cve_column].astype(str).str.strip()

    # Ensure CVSS scores are numeric and within valid range (0-10)
    df_clean[cvss_column] = pd.to_numeric(df_clean[cvss_column], errors='coerce')
    df_clean = df_clean[(df_clean[cvss_column] >= 0) & (df_clean[cvss_column] <= 10)]

    # Group by ID and aggregate CVEs and CVSS scores into lists
    grouped_data = []

    for id, group in df_clean.groupby(id_column):
        # Get list of CVEs and CVSS scores for this machine
        cve_list = group[cve_column].tolist()
        cvss_list = group[cvss_column].tolist()

        # Remove any empty CVEs (shouldn't happen after cleaning, but safety check)
        valid_pairs = [(cve, cvss) for cve, cvss in zip(cve_list, cvss_list)
                      if cve and str(cve).strip() != '']

        # Only include machines with at least one valid CVE-CVSS pair
        if len(valid_pairs) > 0:
            cve_list = [pair[0] for pair in valid_pairs]
            cvss_list = [pair[1] for pair in valid_pairs]

            # Create severity categories
            severity_categories = []
            for cvss in cvss_list:
                if cvss < 4.0:
                    severity_categories.append(0)  # Low
                elif cvss < 7.0:
                    severity_categories.append(1)  # Medium
                elif cvss < 9.0:
                    severity_categories.append(2)  # High
                else:
                    severity_categories.append(3)  # Critical

            row_data = {
                'ID': id,
                'vuln_list': cve_list,
                'cvss_list': cvss_list,
                'severity_categories': severity_categories,
                'vuln_count': len(cve_list),
                'avg_cvss': np.mean(cvss_list),
                'max_cvss': np.max(cvss_list),
                'critical_count': sum(1 for s in severity_categories if s == 3),
                'high_count': sum(1 for s in severity_categories if s == 2)
            }

            grouped_data.append(row_data)

    # Create new DataFrame with processed data
    processed_df = pd.DataFrame(grouped_data)

    # Compile all unique CVEs
    all_cves = []
    all_cvss_scores = []
    for idx, row in processed_df.iterrows():
        all_cves.extend(row['vuln_list'])
        all_cvss_scores.extend(row['cvss_list'])

    # Get unique CVEs only (remove duplicates)
    unique_cves = list(set(all_cves))
    print(f"Total unique CVEs: {len(unique_cves)}")
    print(f"Total machines: {len(processed_df)}")
    print(f"Average CVEs per machine: {np.mean(processed_df['vuln_count']):.2f}")
    print(f"Average CVSS score: {np.mean(all_cvss_scores):.2f}")
    print(f"CVSS distribution - Low: {sum(1 for s in all_cvss_scores if s < 4.0)}, "
          f"Medium: {sum(1 for s in all_cvss_scores if 4.0 <= s < 7.0)}, "
          f"High: {sum(1 for s in all_cvss_scores if 7.0 <= s < 9.0)}, "
          f"Critical: {sum(1 for s in all_cvss_scores if s >= 9.0)}")

    # Create mapping from CVE string to numerical index
    cve_to_idx = {cve: idx for idx, cve in enumerate(unique_cves)}

    # Create reverse mapping from numerical index back to CVE string
    idx_to_cve = {idx: cve for cve, idx in cve_to_idx.items()}

    # Fit CVSS scaler for normalization
    cvss_scaler = StandardScaler()
    cvss_scaler.fit(np.array(all_cvss_scores).reshape(-1, 1))

    return processed_df, cve_to_idx, idx_to_cve, cvss_scaler

# Load the actual file - UPDATE THIS PATH FOR YOUR NEW DATA FORMAT
df = load_data('stripped_grouped_output.csv')  # Update this filename

# Process vulnerabilities using the new format with CVSS integration
df, cve_to_idx, idx_to_cve, cvss_scaler = preprocess_vulnerabilities_with_cvss(df)

# store # of unique CVEs as vocab_size for model architecture
vocab_size = len(cve_to_idx)

def create_training_data_with_cvss(df, cve_to_idx, cvss_scaler, max_set_size=75):
    """
    Create training data with CVSS scores where one CVE is 'masked' and needs to be predicted

    Args:
        df: pandas DataFrame with vulnerability data (processed format)
        cve_to_idx: mapping from CVE strings to indices
        cvss_scaler: fitted StandardScaler for CVSS normalization
        max_set_size: maximum number of CVEs per training example (for padding)

    Returns:
        X_input_cve: numpy array of input CVE indices (with one CVE masked)
        X_input_cvss: numpy array of input CVSS scores (normalized)
        X_input_severity: numpy array of input severity categories
        y_target: numpy array of target CVEs that were masked
    """

    # Initialize lists to store training examples
    X_input_cve = []      # stores CVE indices
    X_input_cvss = []     # stores normalized CVSS scores
    X_input_severity = [] # stores severity categories
    y_target = []         # stores the target CVE that was removed

    for _, row in df.iterrows():
        # Get the lists for this machine
        vuln_list = row['vuln_list']
        cvss_list = row['cvss_list']
        severity_list = row['severity_categories']

        # Skip machines with less than 2 vulnerabilities
        if len(vuln_list) < 2:
            continue

        # Create up to 3 training examples per machine
        for _ in range(min(3, len(vuln_list))):
            if len(vuln_list) >= 2:
                # Randomly select 1 CVE to mask
                masked_idx = random.randint(0, len(vuln_list) - 1)
                # Hidden one "target"
                target_cve = vuln_list[masked_idx]
                target_cvss = cvss_list[masked_idx]

                # Create input sets (all except the masked one)
                input_cves = vuln_list[:masked_idx] + vuln_list[masked_idx+1:]
                input_cvss_scores = cvss_list[:masked_idx] + cvss_list[masked_idx+1:]
                input_severities = severity_list[:masked_idx] + severity_list[masked_idx+1:]

                # Convert CVE strings to indices
                input_cve_indices = [
                    cve_to_idx[cve] for cve in input_cves if cve in cve_to_idx
                ]

                # Normalize CVSS scores
                if len(input_cvss_scores) > 0:
                    input_cvss_normalized = cvss_scaler.transform(
                        np.array(input_cvss_scores).reshape(-1, 1)
                    ).flatten()
                else:
                    input_cvss_normalized = []

                # Convert target CVE to index
                target_idx = cve_to_idx.get(target_cve, -1)

                # Add this example if valid
                if target_idx != -1 and len(input_cve_indices) > 0:
                    # Handle sequence length: pad or truncate to max_set_size
                    if len(input_cve_indices) > max_set_size:
                        input_cve_indices = input_cve_indices[:max_set_size]
                        input_cvss_normalized = input_cvss_normalized[:max_set_size]
                        input_severities = input_severities[:max_set_size]
                    else:
                        # Pad with zeros
                        padding_needed = max_set_size - len(input_cve_indices)
                        input_cve_indices.extend([0] * padding_needed)
                        input_cvss_normalized = list(input_cvss_normalized) + [0.0] * padding_needed
                        input_severities.extend([0] * padding_needed)

                    # Add training example to lists
                    X_input_cve.append(input_cve_indices)
                    X_input_cvss.append(input_cvss_normalized)
                    X_input_severity.append(input_severities)
                    y_target.append(target_idx)

    # Convert lists to numpy arrays
    return (np.array(X_input_cve), np.array(X_input_cvss),
            np.array(X_input_severity), np.array(y_target))

# Create the training data with CVSS features
X_train_cve, X_train_cvss, X_train_severity, y_train = create_training_data_with_cvss(df, cve_to_idx, cvss_scaler)
print(f"Training examples: {len(X_train_cve)}")
print(f"CVE input shape: {X_train_cve.shape}")
print(f"CVSS input shape: {X_train_cvss.shape}")
print(f"Severity input shape: {X_train_severity.shape}")

class MultiHeadAttention(tf.keras.layers.Layer):
    """Multi-head attention mechanism for processing sets of vulnerabilities"""

    def __init__(self, embed_dim, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        assert embed_dim % num_heads == 0
        self.projection_dim = embed_dim // num_heads
        self.query_dense = tf.keras.layers.Dense(embed_dim)
        self.key_dense = tf.keras.layers.Dense(embed_dim)
        self.value_dense = tf.keras.layers.Dense(embed_dim)
        self.combine_heads = tf.keras.layers.Dense(embed_dim)

    def attention(self, query, key, value):
        score = tf.matmul(query, key, transpose_b=True)
        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)
        scaled_score = score / tf.math.sqrt(dim_key)
        weights = tf.nn.softmax(scaled_score, axis=-1)
        output = tf.matmul(weights, value)
        return output, weights

    def separate_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, inputs):
        batch_size = tf.shape(inputs)[0]
        query = self.query_dense(inputs)
        key = self.key_dense(inputs)
        value = self.value_dense(inputs)

        query = self.separate_heads(query, batch_size)
        key = self.separate_heads(key, batch_size)
        value = self.separate_heads(value, batch_size)

        attention, weights = self.attention(query, key, value)
        attention = tf.transpose(attention, perm=[0, 2, 1, 3])
        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))
        output = self.combine_heads(concat_attention)
        return output

class SetAttentionBlock(tf.keras.layers.Layer):
    """Transformer block with multi-head attention and feed-forward network"""

    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(SetAttentionBlock, self).__init__()
        self.att = MultiHeadAttention(embed_dim, num_heads)
        self.ffn = tf.keras.Sequential([
            tf.keras.layers.Dense(ff_dim, activation="relu"),
            tf.keras.layers.Dense(embed_dim),
        ])
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)

    def call(self, inputs, training=None):
        attn_output = self.att(inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

def create_cvss_enhanced_model(vocab_size, embed_dim=256, num_heads=4, ff_dim=1536,
                              num_transformer_blocks=2, max_set_size=75):
    """
    Create Set Transformer model enhanced with CVSS score features

    This model processes three types of input:
    1. CVE embeddings (learned representations)
    2. CVSS scores (normalized numerical features)
    3. Severity categories (categorical features)
    """

    # Input layers
    cve_input = tf.keras.layers.Input(shape=(max_set_size,), name='cve_input')
    cvss_input = tf.keras.layers.Input(shape=(max_set_size,), name='cvss_input')
    severity_input = tf.keras.layers.Input(shape=(max_set_size,), name='severity_input')

    # CVE Embedding layer
    cve_embedding = tf.keras.layers.Embedding(vocab_size + 1, embed_dim)(cve_input)

    # CVSS feature processing
    cvss_expanded = tf.keras.layers.Reshape((max_set_size, 1))(cvss_input)
    cvss_dense = tf.keras.layers.Dense(embed_dim // 4, activation='relu')(cvss_expanded)

    # Severity category embedding (4 categories: Low, Medium, High, Critical)
    severity_embedding = tf.keras.layers.Embedding(5, embed_dim // 4)(severity_input)

    # Combine all features
    # Concatenate CVSS and severity features
    combined_features = tf.keras.layers.Concatenate(axis=-1)([cvss_dense, severity_embedding])

    # Project combined features to match embedding dimension
    feature_projection = tf.keras.layers.Dense(embed_dim // 2)(combined_features)

    # Combine with CVE embeddings
    combined_input = tf.keras.layers.Concatenate(axis=-1)([cve_embedding, feature_projection])

    # Project to final embedding dimension
    x = tf.keras.layers.Dense(embed_dim)(combined_input)

    # Create mask using MaskingLayer
    mask_layer = MaskingLayer()
    mask = mask_layer(cve_input)
    # mask shape: (batch_size, max_set_size, 1)


    # Apply mask to embeddings
    x = x * mask

    # Apply transformer blocks
    for block_idx in range(num_transformer_blocks):
        transformer_block = SetAttentionBlock(embed_dim, num_heads, ff_dim)
        x = transformer_block(x)
        x = x * mask  # Re-apply mask to ensure padding remains zero

    # Global average pooling (accounting for variable lengths)
    set_lengths = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))(mask)
    x = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))(x)
    x = tf.keras.layers.Lambda(lambda x: x[0] / (x[1] + tf.keras.backend.epsilon()))([x, set_lengths])

    # Classification layers
    x = tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)
    x = tf.keras.layers.Dropout(0.3)(x)

    x = tf.keras.layers.Dense(ff_dim, activation='relu')(x)
    x = tf.keras.layers.Dropout(0.2)(x)

    x = tf.keras.layers.Dense(ff_dim // 2, activation='relu')(x)
    x = tf.keras.layers.Dropout(0.2)(x)

    # Output layer
    outputs = tf.keras.layers.Dense(vocab_size, activation='softmax')(x)

    # Create model with multiple inputs
    model = tf.keras.Model(inputs=[cve_input, cvss_input, severity_input], outputs=outputs)

    return model

class MaskingLayer(tf.keras.layers.Layer):
    def call(self, inputs):
        mask = tf.cast(tf.not_equal(inputs, 0), tf.float32)
        mask = tf.expand_dims(mask, axis=-1)
        return mask

#maybe mess with max set size, as in, I believe some machines has 100+ vulns
#also worth experimenting with embed_dim=128, num_heads=8, ff_dim=512, embed_dim=64, num_heads=4 for example
def create_set_transformer_model(vocab_size, embed_dim=256, num_heads=4, ff_dim=1536,
                                num_transformer_blocks=2, max_set_size=75):
    """
    Actually creating Set Transformer model for vulnerability co-occurrence prediction

    building with:
    1. Embedding layer (converts CVE indices to dense vectors)
    2. Multiple Set Attention Blocks (the core Transformer processing)
    3. Pooling layer (combines variable-length sets into fixed-size vectors)
    4. Classification layers (predicts which CVE was masked)

    Args:
        vocab_size: number of unique CVEs in vocabulary
        embed_dim: dimension of embedding vectors
        num_heads: number of attention heads per block
        ff_dim: dimension of feed-forward networks
        num_transformer_blocks: number of transformer layers to stack
        max_set_size: maximum number of CVEs per input set

    Returns:
        compiled Keras model ready for training
    """

    # Define input layer: expects sequences of integers (CVE indices)
    # Shape: (batch_size, max_set_size)
    inputs = tf.keras.layers.Input(shape=(max_set_size,))

    # Embedding layer: converts integer indices to dense vectors
    # vocab_size + 1 bc index 0 is for padding
    # Each CVE gets mapped to a dense vector of size embed_dim
    embedding_layer = tf.keras.layers.Embedding(vocab_size + 1, embed_dim)
    x = embedding_layer(inputs)
    # x shape: (batch_size, max_set_size, embed_dim)

    # Create mask using MaskingLayer
    mask_layer = MaskingLayer()
    mask = mask_layer(inputs)
    # mask shape: (batch_size, max_set_size, 1)

    # Apply mask to embeddings (also set padding positions to zero)
    x = x * mask

    # each block does more and more pattern learning
    for block_idx in range(num_transformer_blocks):
        transformer_block = SetAttentionBlock(embed_dim, num_heads, ff_dim)
        x = transformer_block(x)
        x = x * mask #going to re-apply the mask to make sure padding = zero

#a lot from docs again here
    set_lengths = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))(mask)

    x = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))(x)

    x = tf.keras.layers.Lambda(lambda x: x[0] / (x[1] + tf.keras.backend.epsilon()))([x, set_lengths])

    # First hidden layer with ReLU activation
    x = tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)

    x = tf.keras.layers.Dense(ff_dim, activation='relu')(x)

    # Add dropout for regularization
    x = tf.keras.layers.Dropout(0.2)(x) # test out 0.3â€“0.5

    # Second hidden layer (smaller)
    x = tf.keras.layers.Dense(ff_dim // 2, activation='relu')(x)
    # Add more dropout
    x = tf.keras.layers.Dropout(0.2)(x)

    # Output layer: predict probability distribution over all possible CVEs
    # vocab_size outputs (one for each possible CVE)
    # Softmax activation ensures outputs sum to 1 (valid probability distribution)
    outputs = tf.keras.layers.Dense(vocab_size, activation='softmax')(x)

    # Create the complete model
    model = tf.keras.Model(inputs, outputs)

    return model

# Create the enhanced model
model = create_cvss_enhanced_model(vocab_size)

# Learning rate schedule
initial_learning_rate = 0.00010980988314820266
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate,
    decay_steps=10000,
    decay_rate=0.9,
    staircase=True
)

# Compile the model
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy'] # Removed TopKCategoricalAccuracy for debugging
)

# Print model summary
print(model.summary())

# Split training data
(X_train_cve_split, X_val_cve_split,
 X_train_cvss_split, X_val_cvss_split,
 X_train_severity_split, X_val_severity_split,
 y_train_split, y_val_split) = train_test_split(
    X_train_cve, X_train_cvss, X_train_severity, y_train,
    test_size=0.2,
    random_state=42
)

# Ensure y_train_split is a 1D numpy array of integers and cast to tf.int64
y_train_split = np.array(y_train_split).flatten().astype(np.int64)
y_val_split = np.array(y_val_split).flatten().astype(np.int64)


print(f"Training set: {len(X_train_cve_split)} examples")
print(f"Validation set: {len(X_val_cve_split)} examples")

# Train the model
history = model.fit(
    [X_train_cve_split, X_train_cvss_split, X_train_severity_split],
    y_train_split,
    batch_size=16,
    epochs=60,
    validation_data=([X_val_cve_split, X_val_cvss_split, X_val_severity_split], y_val_split),
    callbacks=[
        tf.keras.callbacks.EarlyStopping(
            patience=10,
            restore_best_weights=True
        )
    ]
)

# Evaluation
from sklearn.metrics import classification_report

# Predict on validation set
y_pred = model.predict([X_val_cve_split, X_val_cvss_split, X_val_severity_split]).argmax(axis=1)
print(classification_report(y_val_split, y_pred))

# Additional evaluation metrics
print("\n=== Model Performance Analysis ===")
print(f"Top-1 Accuracy: {np.mean(y_pred == y_val_split):.4f}")

# Calculate top-5 accuracy manually
y_pred_probs = model.predict([X_val_cve_split, X_val_cvss_split, X_val_severity_split])
top_5_predictions = np.argsort(y_pred_probs, axis=1)[:, -5:]
top_5_accuracy = np.mean([y_val_split[i] in top_5_predictions[i] for i in range(len(y_val_split))])
print(f"Top-5 Accuracy: {top_5_accuracy:.4f}")