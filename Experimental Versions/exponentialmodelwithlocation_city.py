# -*- coding: utf-8 -*-
"""ExponentialModelWithLocation_City.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BK0VWNg932Kp7Yya9bIhS6M3TCKBjZLd
"""

pip install tensorflow pandas openpyxl scikit-learn numpy matplotlib

import pandas as pd              # Data analysis and handling a .csv or .xlsx
import numpy as np              # For math and handling arrays
import tensorflow as tf         # deep learning framework
from tensorflow.keras import regularizers
from sklearn.preprocessing import LabelEncoder  # labelling categorical data (CVE and Location)
from sklearn.model_selection import train_test_split  # for splitting the data
import random                   # needed for masking
from collections import Counter # needed for counting in this model

def load_data(file_path):
    # we want either a .csv or .xlsx, so we must check it first
    if file_path.endswith('.csv'):
        # use pandas to put the data from the .csv into a df
        # just incase any wonky data (probably not a problem? Maybe delete?), on_bad_lines='skip'
        df = pd.read_csv(file_path, engine='python')
    elif file_path.endswith('.xlsx'):
        # use pandas to put the data from the .xlsx into a df
        df = pd.read_excel(file_path)
    else:
        # error
        raise ValueError("Upload .csv or .xlsx only")

    # return loaded DataFrame
    return df

def preprocess_vulnerabilities_and_locations(df, vulnerability_column='Vulnerabilities', location_column='Location'):
    """
    This function takes the comma-separated strings of CVEs and location data:
    1. Converts CVE strings to lists
    2. Creates a list of all unique CVEs
    3. Creates bidirectional mappings between CVEs and numerical indices
    4. Encodes locations as numerical indices
    5. Creates bidirectional mappings for locations

    Args:
        df: pandas DataFrame containing the data
        vulnerability_column: name of column containing vulnerability strings
        location_column: name of column containing location data

    Returns:
        df: DataFrame with added columns -> 'vuln_list' and 'location_encoded'
        cve_to_idx: dictionary mapping CVE strings to indices
        idx_to_cve: dictionary mapping indices to CVE strings
        location_to_idx: dictionary mapping location strings to indices
        idx_to_location: dictionary mapping indices to location strings
    """

    # Convert string of CVEs to lists for each row for formatting
    # lambda function processes each cell in the vulnerability column
    df['vuln_list'] = df[vulnerability_column].apply(
        lambda x: [
            cve.strip()  # Removes whitespace
            for cve in str(x).split(' ')  # Split by comma to get individual CVEs
            if cve.strip()  # Only keep non-empty CVEs after stripping whitespace
        ]
    )

    # Compile all unique CVEs
    all_cves = []
    # Iterate through each machine's vulnerability list
    for vuln_list in df['vuln_list']:
        # Add all vulnerabilities from this machine to the master list
        all_cves.extend(vuln_list)

    # Get unique CVEs only (remove duplicates by converting to set, then back to list)
    unique_cves = list(set(all_cves))
    # Print what we have so far
    print(f"Total unique CVEs: {len(unique_cves)}")

    # Create mapping from CVE string to numerical index
    # needed because neural networks work with numbers, basically just formatting
    cve_to_idx = {cve: idx for idx, cve in enumerate(unique_cves)}

    # Create reverse mapping from numerical index back to CVE string
    idx_to_cve = {idx: cve for cve, idx in cve_to_idx.items()}

    # Process locations
    # Handle missing locations by filling with 'Unknown'
    df[location_column] = df[location_column].fillna('Unknown').astype(str)

    # Get unique locations
    unique_locations = df[location_column].unique().tolist()
    print(f"Total unique locations: {len(unique_locations)}")

    # Create mappings for locations
    location_to_idx = {location: idx for idx, location in enumerate(unique_locations)}
    idx_to_location = {idx: location for location, idx in location_to_idx.items()}

    # Encode locations as numerical indices
    df['location_encoded'] = df[location_column].map(location_to_idx)

    # Return processed DataFrame and all mappings
    return df, cve_to_idx, idx_to_cve, location_to_idx, idx_to_location

# the actual file (**Change this when uploading different file)
df = load_data('Dataset_Additional_output_Cleaned.csv')

# recursive call with my actual file - now includes location processing
df, cve_to_idx, idx_to_cve, location_to_idx, idx_to_location = preprocess_vulnerabilities_and_locations(df)

# store # of unique CVEs and locations for model architecture
vocab_size = len(cve_to_idx)
num_locations = len(location_to_idx)

def create_training_data_with_location(df, cve_to_idx, max_set_size=75):
    """
    Create training data where one CVE is 'masked' and needs to be predicted,
    now incorporating location information as an additional input feature.

    Args:
        df: pandas DataFrame with vulnerability and location data
        cve_to_idx: mapping from CVE strings to indices
        max_set_size: maximum number of CVEs per training example (for padding)

    Returns:
        X_input_vulns: numpy array of input vulnerability sets (with one CVE masked)
        X_input_locations: numpy array of location indices for each example
        y_target: numpy array of target CVEs that were masked
    """

    # Initialize lists to store training examples
    X_input_vulns = []      # stores input sets (vulnerability lists with 1 CVE removed)
    X_input_locations = []  # stores location indices for each example
    y_target = []           # stores the target CVE that was removed (what we want predicted)

    for _, row in df.iterrows():
        # Get the list of vulnerabilities and location for each row
        vuln_list = row['vuln_list']
        location_idx = row['location_encoded']

        # Skip machines with less than 2 vulnerabilities
        if len(vuln_list) < 2:
            continue

        # creating up to 3 examples per machine
        for _ in range(min(3, len(vuln_list))):
            # Double-check the # of the vulns
            if len(vuln_list) >= 2:
                # Randomly select 1 CVE to mask
                masked_idx = random.randint(0, len(vuln_list) - 1)
                # Hidden one "target"
                target_cve = vuln_list[masked_idx]

                # Create input set (all CVEs except the masked one)
                input_cves = vuln_list[:masked_idx] + vuln_list[masked_idx+1:]

                # Convert CVE strings to indices bc models need numbers, not strings
                input_indices = [
                    cve_to_idx[cve]
                    for cve in input_cves
                    if cve in cve_to_idx  # Only CVEs that exist in the list
                ]

                # Convert target CVE also to index, -1 if not found
                target_idx = cve_to_idx.get(target_cve, -1)

                # add this example if both target and input are valid (not -1 or something)
                if target_idx != -1 and len(input_indices) > 0:
                    # Handle sequence length: pad or truncate to max_set_size
                    if len(input_indices) > max_set_size:
                        # If too many vulnerabilities, keep only the first max_set_size
                        input_indices = input_indices[:max_set_size]
                    else:
                        # If too few vulnerabilities, pad with zeros
                        padding_needed = max_set_size - len(input_indices)
                        input_indices.extend([0] * padding_needed)

                    # add training example to lists
                    X_input_vulns.append(input_indices)
                    X_input_locations.append(location_idx)
                    y_target.append(target_idx)

    # Convert lists to numpy arrays (required for TensorFlow)
    return np.array(X_input_vulns), np.array(X_input_locations), np.array(y_target)

# Create the training data using masking (recursive call) - now with location
X_train_vulns, X_train_locations, y_train = create_training_data_with_location(df, cve_to_idx)
# Print stats about training data
print(f"Training examples: {len(X_train_vulns)}")

class MultiHeadAttention(tf.keras.layers.Layer):
    """
    Basically allows the model to focus on different parts of the input set at the same time
    """

    def __init__(self, embed_dim, num_heads):
        """
        Initialize Multi-Head Attention layer

        Args:
            embed_dim: dimensionality of the embedding space
            num_heads: number of attention heads to use
        """
        # Call parent class
        super(MultiHeadAttention, self).__init__()

        self.embed_dim = embed_dim
        self.num_heads = num_heads
        assert embed_dim % num_heads == 0
        self.projection_dim = embed_dim // num_heads
        self.query_dense = tf.keras.layers.Dense(embed_dim)
        self.key_dense = tf.keras.layers.Dense(embed_dim)
        self.value_dense = tf.keras.layers.Dense(embed_dim)
        self.combine_heads = tf.keras.layers.Dense(embed_dim)

    def attention(self, query, key, value):
        """
        Args:
            query: what we're looking for
            key: what we're looking in
            value: what we actually return

        Returns:
            output: attention-weighted values
            weights: attention weights (for interpretability)
        """
        score = tf.matmul(query, key, transpose_b=True)
        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)
        scaled_score = score / tf.math.sqrt(dim_key)
        weights = tf.nn.softmax(scaled_score, axis=-1)
        output = tf.matmul(weights, value)
        return output, weights

    def separate_heads(self, x, batch_size):
        """
        Args:
            x: input tensor
            batch_size: size of the current batch

        Returns:
            reshaped tensor with separate heads
        """
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, inputs):
        """
        Forward pass of the Multi-Head Attention layer
        """
        batch_size = tf.shape(inputs)[0]
        query = self.query_dense(inputs)
        key = self.key_dense(inputs)
        value = self.value_dense(inputs)
        query = self.separate_heads(query, batch_size)
        key = self.separate_heads(key, batch_size)
        value = self.separate_heads(value, batch_size)
        attention, weights = self.attention(query, key, value)
        attention = tf.transpose(attention, perm=[0, 2, 1, 3])
        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))
        output = self.combine_heads(concat_attention)
        return output

class SetAttentionBlock(tf.keras.layers.Layer):
    """
    A complete Transformer block combining Multi-Head Attention with Feed-Forward Network
    """

    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(SetAttentionBlock, self).__init__()
        self.att = MultiHeadAttention(embed_dim, num_heads)
        self.ffn = tf.keras.Sequential([
            tf.keras.layers.Dense(ff_dim, activation="relu"),
            tf.keras.layers.Dense(embed_dim),
        ])
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)

    def call(self, inputs, training=None):
        attn_output = self.att(inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

class MaskingLayer(tf.keras.layers.Layer):
    def call(self, inputs):
        mask = tf.cast(tf.not_equal(inputs, 0), tf.float32)
        mask = tf.expand_dims(mask, axis=-1)
        return mask

def create_set_transformer_model_with_location(vocab_size, num_locations, embed_dim=256,
                                             location_embed_dim=32, num_heads=4, ff_dim=1536,
                                             num_transformer_blocks=2, max_set_size=75):
    """
    Create Set Transformer model for vulnerability co-occurrence prediction with location information

    Args:
        vocab_size: number of unique CVEs in vocabulary
        num_locations: number of unique locations
        embed_dim: dimension of CVE embedding vectors
        location_embed_dim: dimension of location embedding vectors
        num_heads: number of attention heads per block
        ff_dim: dimension of feed-forward networks
        num_transformer_blocks: number of transformer layers to stack
        max_set_size: maximum number of CVEs per input set

    Returns:
        compiled Keras model ready for training
    """

    # Define input layers
    # Vulnerability input: sequences of integers (CVE indices)
    vuln_inputs = tf.keras.layers.Input(shape=(max_set_size,), name='vulnerability_input')
    # Location input: single integer (location index)
    location_inputs = tf.keras.layers.Input(shape=(), name='location_input')

    # CVE Embedding layer: converts integer indices to dense vectors
    # vocab_size + 1 bc index 0 is for padding
    cve_embedding_layer = tf.keras.layers.Embedding(vocab_size + 1, embed_dim, name='cve_embedding')
    vuln_embeddings = cve_embedding_layer(vuln_inputs)
    # vuln_embeddings shape: (batch_size, max_set_size, embed_dim)

    # Location Embedding layer: converts location indices to dense vectors
    location_embedding_layer = tf.keras.layers.Embedding(num_locations, location_embed_dim, name='location_embedding')
    location_embeddings = location_embedding_layer(location_inputs)
    # location_embeddings shape: (batch_size, location_embed_dim)

    # Create mask for vulnerabilities
    mask_layer = MaskingLayer()
    mask = mask_layer(vuln_inputs)
    # mask shape: (batch_size, max_set_size, 1)

    # Apply mask to vulnerability embeddings
    masked_vuln_embeddings = vuln_embeddings * mask

    # Process vulnerabilities through transformer blocks
    x = masked_vuln_embeddings
    for block_idx in range(num_transformer_blocks):
        transformer_block = SetAttentionBlock(embed_dim, num_heads, ff_dim)
        x = transformer_block(x)
        x = x * mask  # re-apply the mask to make sure padding = zero

    # Calculate set lengths for average pooling
    set_lengths = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))(mask)

    # Apply average pooling to get set representation
    vuln_set_representation = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))(x)
    vuln_set_representation = tf.keras.layers.Lambda(
        lambda x: x[0] / (x[1] + tf.keras.backend.epsilon())
    )([vuln_set_representation, set_lengths])
    # vuln_set_representation shape: (batch_size, embed_dim)

    # Expand location embeddings to match batch dimension properly
    location_representation = tf.keras.layers.Lambda(lambda x: tf.squeeze(x, axis=1))(
        tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=1))(location_embeddings)
    )
    # location_representation shape: (batch_size, location_embed_dim)

    # Combine vulnerability set representation with location information
    # Concatenate the two representations
    combined_representation = tf.keras.layers.Concatenate(axis=-1)([
        vuln_set_representation,
        location_representation
    ])
    # combined_representation shape: (batch_size, embed_dim + location_embed_dim)

    # Processing layers
    # First hidden layer with ReLU activation
    x = tf.keras.layers.Dense(128, activation='relu',
                             kernel_regularizer=regularizers.l2(0.001))(combined_representation)

    # Second hidden layer
    x = tf.keras.layers.Dense(ff_dim, activation='relu')(x)

    # Add dropout for regularization
    x = tf.keras.layers.Dropout(0.2)(x)

    # Third hidden layer (smaller)
    x = tf.keras.layers.Dense(ff_dim // 2, activation='relu')(x)

    # Add more dropout
    x = tf.keras.layers.Dropout(0.2)(x)

    # Output layer: predict probability distribution over all possible CVEs
    outputs = tf.keras.layers.Dense(vocab_size, activation='softmax')(x)

    # Create the complete model with multiple inputs
    model = tf.keras.Model(inputs=[vuln_inputs, location_inputs], outputs=outputs)

    return model

model = create_set_transformer_model_with_location(vocab_size, num_locations)

# Exponential decay learning rate schedule
initial_learning_rate = 0.00010980988314820266
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate,
    decay_steps=10000,
    decay_rate=0.9,
    staircase=True)

# Compile the model
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Print model summary
print(model.summary())

# Split training data into separate training and validation sets
X_train_vulns_split, X_val_vulns_split, X_train_locations_split, X_val_locations_split, y_train_split, y_val_split = train_test_split(
    X_train_vulns,      # vulnerability sets with one CVE masked
    X_train_locations,  # location indices
    y_train,            # the masked CVE indices
    test_size=0.2,      # Use 20% of data for validation
    random_state=42     # fixed seed number
)

# Print dataset stats
print(f"Training set: {len(X_train_vulns_split)} examples")
print(f"Validation set: {len(X_val_vulns_split)} examples")

# Training the model with location information
history = model.fit(
    [X_train_vulns_split, X_train_locations_split],  # Multiple inputs: vulnerabilities and locations
    y_train_split,                                   # Training target labels
    batch_size=16,                                   # 16 examples at a time
    epochs=60,                                       # number of epochs
    validation_data=([X_val_vulns_split, X_val_locations_split], y_val_split),  # Validation data with multiple inputs
    callbacks=[
        # stop training if validation loss doesn't improve for 10 epochs
        tf.keras.callbacks.EarlyStopping(
            patience=10,                 # Wait 10 epochs before stopping
            restore_best_weights=True    # Restore weights from best epoch
        )
    ]
)

# Evaluation with classification report
from sklearn.metrics import classification_report

# Predict on validation set with multiple inputs
y_pred = model.predict([X_val_vulns_split, X_val_locations_split]).argmax(axis=1)
print(classification_report(y_val_split, y_pred))