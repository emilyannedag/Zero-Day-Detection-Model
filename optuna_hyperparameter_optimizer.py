# -*- coding: utf-8 -*-
"""Optuna_Hyperparameter_Optimizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EH5hjriIT9SP9PGFhFfA1pxLKfbg9WKf
"""

!pip install optuna

import optuna
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import regularizers
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import random
from collections import Counter
from sklearn.metrics import classification_report
import gc  # For garbage collection

def load_data(file_path):

    # we want either a .csv or .xlsx, so we must check it first
    if file_path.endswith('.csv'):
        # use pandas to put the data from the .csv into a df
        # just incase any wonky data (probably not a problem? Maybe delete?), on_bad_lines='skip'
        df = pd.read_csv(file_path, engine='python')
    elif file_path.endswith('.xlsx'):
        # use pandas to put the data from the .xlsx into a df
        df = pd.read_excel(file_path)
    else:
        # error
        raise ValueError("Upload .csv or .xlsx only")

    # return loaded DataFrame
    return df

def preprocess_vulnerabilities(df, vulnerability_column='Vulnerabilities'):
    """

    This function takes the comma-separated strings of CVEs and:
    1. Converts them to lists
    2. Creates a list of all unique CVEs
    3. Creates bidirectional mappings between CVEs and numerical indices, by Google's advice

    Args:
        df: pandas DataFrame containing the data
        vulnerability_column: name of column containing vulnerability strings

    Returns:
        df: DataFrame with an added column -> 'vuln_list'
        cve_to_idx: dictionary mapping CVE strings to indices
        idx_to_cve: dictionary mapping indices to CVE strings
        ^^actually, unclear why we need the bidirectional mapping?
    """

    # Convert string of CVEs to lists for each row for formatting
    # lambda function processes each cell in the vulnerability column
    df['vuln_list'] = df[vulnerability_column].apply(
        lambda x: [
            cve.strip()  # Removes whitespace
            for cve in str(x).split(',')  # Split by comma to get individual CVEs
            if cve.strip()  # Only keep non-empty CVEs after stripping whitespace, although this shouldnt be a problem
        ]
    )

    # Compile all unique CVEs
    all_cves = []
    # Iterate through each machine's vulnerability list
    for vuln_list in df['vuln_list']:
        # Add all vulnerabilities from this machine to the master list
        all_cves.extend(vuln_list)

    # Get unique CVEs only (remove duplicates by converting to set, then back to list)
    unique_cves = list(set(all_cves))
    # Print what we have so far
    print(f"Total unique CVEs: {len(unique_cves)}")

    # Create mapping from CVE string to numerical index
    # needed because neural networks work with numbers, basically just formatting
    cve_to_idx = {cve: idx for idx, cve in enumerate(unique_cves)}

    # Create reverse mapping from numerical index back to CVE string
    # Google says its for interpreting model predictions, but I dont get this one
    idx_to_cve = {idx: cve for cve, idx in cve_to_idx.items()}

    # Return processed DataFrame and both mappings
    return df, cve_to_idx, idx_to_cve

# the actual file (**Change this when uploading different file)
df = load_data('3000sample_plain.csv')

# recursive call with my actual file
df, cve_to_idx, idx_to_cve = preprocess_vulnerabilities(df)

# store # of unique CVEs as vocab_size for model architecture
vocab_size = len(cve_to_idx)

def create_training_data(df, cve_to_idx, max_set_size=50):
    """
    Create training data where one CVE is 'masked' and needs to be predicted

    training strategy: teach the model to predict missing vulnerabilities by showing it incomplete sets and asking it to guess what's missing.

    Args:
        df: DataFrame with vulnerability data
        cve_to_idx: mapping from CVE strings to indices
        max_set_size: maximum number of CVEs per training example (for padding)

    Returns:
        X_input: numpy array of input vulnerability sets (with one CVE masked)
        y_target: numpy array of target CVEs that were masked
    """

    # Initialize lists to store training examples
    X_input = []   # stores input sets (vulnerability lists with 1 CVE removed)
    y_target = []  # stores the target CVE that was removed (what we want predicted)


    for _, row in df.iterrows():
        # Get the list of vulnerabilities for each row
        vuln_list = row['vuln_list']

        # Skip machines with less than 2, **maybe delete this? dont need it anymore
        if len(vuln_list) < 2:
            continue

        # creating up to 3 examples per machine
        for _ in range(min(3, len(vuln_list))):
            # Double-check the # of the vulns (maybe delete?)
            if len(vuln_list) >= 2:
                # Randomly select 1 CVE to mask
                masked_idx = random.randint(0, len(vuln_list) - 1)
                # Hidden one "target"
                target_cve = vuln_list[masked_idx]

                # Create input set (all CVEs except the masked one)
                input_cves = vuln_list[:masked_idx] + vuln_list[masked_idx+1:]

                # Convert CVE strings to indices bc models need numbers, not strings
                # check (cve in cve_to_idx)
                input_indices = [
                    cve_to_idx[cve]
                    for cve in input_cves
                    if cve in cve_to_idx  # Only CVEs that exist in the list
                ]

                # Convert target CVE also to index, -1 if not found
                target_idx = cve_to_idx.get(target_cve, -1)

                # add this example if both target and input are valid (not -1 or something)
                if target_idx != -1 and len(input_indices) > 0:
                    # Handle sequence length: pad or truncate to max_set_size
                    if len(input_indices) > max_set_size:
                        # If too many vulnerabilities, keep only the first max_set_size
                        input_indices = input_indices[:max_set_size]
                    else:
                        # If too few vulnerabilities, pad with zeros
                        padding_needed = max_set_size - len(input_indices)
                        input_indices.extend([0] * padding_needed)

                    # add training example to lists
                    X_input.append(input_indices)
                    y_target.append(target_idx)

    # Convert lists to numpy arrays (required for TensorFlow)
    return np.array(X_input), np.array(y_target)

# Create the training data using masking (recursive call)
X_train, y_train = create_training_data(df, cve_to_idx)
# Print stats about training data
print(f"Training examples: {len(X_train)}")

class MultiHeadAttention(tf.keras.layers.Layer):
    """
    Basically allows the model to focus on different parts of the input set at the same time
    """

    def __init__(self, embed_dim, num_heads):
        """
        Initialize Multi-Head Attention layer

        Args:
            embed_dim: dimensionality of the embedding space
            num_heads: number of attention heads to use

        Admittedly, I got most of this from keras documentation/guides
        used to learn set-based representations of CVEs
        """
        # Call parent class
        super(MultiHeadAttention, self).__init__()

        # from documentation (shrug)
        self.embed_dim = embed_dim
        self.num_heads = num_heads

        # from documentation (shrug)
        assert embed_dim % num_heads == 0

        # from documentation (shrug)
        self.projection_dim = embed_dim // num_heads

        # from documentation (shrug)
        self.query_dense = tf.keras.layers.Dense(embed_dim)
        self.key_dense = tf.keras.layers.Dense(embed_dim)
        self.value_dense = tf.keras.layers.Dense(embed_dim)

        # from documentation (shrug)
        self.combine_heads = tf.keras.layers.Dense(embed_dim)

    def attention(self, query, key, value):
        """
        Args:
            query: what we're looking for
            key: what we're looking in
            value: what we actually return

        Returns:
            output: attention-weighted values
            weights: attention weights (for interpretability)
            All basically from documentation/guides VVV
        """

        score = tf.matmul(query, key, transpose_b=True)

        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)

        scaled_score = score / tf.math.sqrt(dim_key)

        weights = tf.nn.softmax(scaled_score, axis=-1)

        output = tf.matmul(weights, value)

        return output, weights

    def separate_heads(self, x, batch_size):
        """
 From documentation
        Args:
            x: input tensor
            batch_size: size of the current batch

        Returns:
            reshaped tensor with separate heads
        """
        # Reshaping for tf
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))

        # Transposing for tf
        # grouping all heads together bc efficient I think
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, inputs):
        """
        Forward pass of the Multi-Head Attention layer

        Args:
            inputs: input tensor

        Returns:
            output tensor after applying multi-head attention
        """
        # Get batch size from input shape
        batch_size = tf.shape(inputs)[0]

        # Transform inputs to queries, keys, and values using learned linear transformations
        query = self.query_dense(inputs)
        key = self.key_dense(inputs)
        value = self.value_dense(inputs)

        # Separate each into multiple heads for parallel attention
        query = self.separate_heads(query, batch_size)
        key = self.separate_heads(key, batch_size)
        value = self.separate_heads(value, batch_size)

        # Apply attention mechanism
        attention, weights = self.attention(query, key, value)

        # Transpose back: (batch_size, num_heads, seq_length, projection_dim) -> (batch_size, seq_length, num_heads, projection_dim)
        attention = tf.transpose(attention, perm=[0, 2, 1, 3])

        # Concatenate all heads: (batch_size, seq_length, num_heads, projection_dim) -> (batch_size, seq_length, embed_dim)
        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))

        # Apply final linear transformation to combine information from all heads
        output = self.combine_heads(concat_attention)

        return output

class SetAttentionBlock(tf.keras.layers.Layer):
    """
    A complete Transformer block combining Multi-Head Attention with Feed-Forward Network

    This is one "layer" of the Transformer. It includes:
    1. Multi-head attention (to find relationships between vulnerabilities)
    2. Feed-forward network (to process the attended information)
    3. Residual connections (to help with training deep networks)
    4. Layer normalization (to stabilize training)
    """

    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        """
        More from docs

        Args:
            embed_dim: embedding dimension
            num_heads: number of attention heads
            ff_dim: dimension of feed-forward network
            rate: dropout rate for regularization
        """
        super(SetAttentionBlock, self).__init__()


        self.att = MultiHeadAttention(embed_dim, num_heads)


        self.ffn = tf.keras.Sequential([
            tf.keras.layers.Dense(ff_dim, activation="relu"),
            tf.keras.layers.Dense(embed_dim),
        ])


        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

        #prevent overfitting
        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)

    def call(self, inputs, training=None):
        """
        More from docs

        Args:
            inputs: input tensor
            training: whether we're in training mode (affects dropout)
                     If None, Keras will automatically determine based on context

        Returns:
            output tensor after applying attention and feed-forward processing
        """

        attn_output = self.att(inputs)


        attn_output = self.dropout1(attn_output, training=training)


        out1 = self.layernorm1(inputs + attn_output)


        ffn_output = self.ffn(out1)


        ffn_output = self.dropout2(ffn_output, training=training)


        return self.layernorm2(out1 + ffn_output)

class MaskingLayer(tf.keras.layers.Layer):
    def call(self, inputs):
        mask = tf.cast(tf.not_equal(inputs, 0), tf.float32)
        mask = tf.expand_dims(mask, axis=-1)
        return mask
#maybe mess with max set size, as in, I believe some machines has 100+ vulns
#also worth experimenting with embed_dim=128, num_heads=8, ff_dim=512, embed_dim=64, num_heads=4 for example
def create_set_transformer_model(vocab_size, embed_dim=64, num_heads=4, ff_dim=512,
                                num_transformer_blocks=4, max_set_size=50):
    """
    Actually creating Set Transformer model for vulnerability co-occurrence prediction

    building with:
    1. Embedding layer (converts CVE indices to dense vectors)
    2. Multiple Set Attention Blocks (the core Transformer processing)
    3. Pooling layer (combines variable-length sets into fixed-size vectors)
    4. Classification layers (predicts which CVE was masked)

    Args:
        vocab_size: number of unique CVEs in vocabulary
        embed_dim: dimension of embedding vectors
        num_heads: number of attention heads per block
        ff_dim: dimension of feed-forward networks
        num_transformer_blocks: number of transformer layers to stack
        max_set_size: maximum number of CVEs per input set

    Returns:
        compiled Keras model ready for training
    """

    # Define input layer: expects sequences of integers (CVE indices)
    # Shape: (batch_size, max_set_size)
    inputs = tf.keras.layers.Input(shape=(max_set_size,))

    # Embedding layer: converts integer indices to dense vectors
    # vocab_size + 1 bc index 0 is for padding
    # Each CVE gets mapped to a dense vector of size embed_dim
    embedding_layer = tf.keras.layers.Embedding(vocab_size + 1, embed_dim)
    x = embedding_layer(inputs)
    # x shape: (batch_size, max_set_size, embed_dim)

    # Create mask using MaskingLayer
    mask_layer = MaskingLayer()
    mask = mask_layer(inputs)
    # mask shape: (batch_size, max_set_size, 1)

    # Apply mask to embeddings (also set padding positions to zero)
    x = x * mask

    # each block does more and more pattern learning
    for block_idx in range(num_transformer_blocks):

        transformer_block = SetAttentionBlock(embed_dim, num_heads, ff_dim)
        x = transformer_block(x)
        x = x * mask #going to re-apply the mask to make sure padding = zero
#a lot from docs again here
    set_lengths = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))(mask)


    x = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))(x)

    x = tf.keras.layers.Lambda(lambda x: x[0] / (x[1] + tf.keras.backend.epsilon()))([x, set_lengths])




    # First hidden layer with ReLU activation
    x = tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)


    x = tf.keras.layers.Dense(ff_dim, activation='relu')(x)

    # Add dropout for regularization
    x = tf.keras.layers.Dropout(0.2)(x) # test out 0.3â€“0.5

    # Second hidden layer (smaller)
    x = tf.keras.layers.Dense(ff_dim // 2, activation='relu')(x)
    # Add more dropout
    x = tf.keras.layers.Dropout(0.2)(x)

    # Output layer: predict probability distribution over all possible CVEs
    # vocab_size outputs (one for each possible CVE)
    # Softmax activation ensures outputs sum to 1 (valid probability distribution)
    outputs = tf.keras.layers.Dense(vocab_size, activation='softmax')(x)

    # Create the complete model
    model = tf.keras.Model(inputs, outputs)

    return model

# Create the model with vocab_size
model = create_set_transformer_model(vocab_size)


# SOME NEW STUFF FOR EXPONENTIAL DECAY ****************
initial_learning_rate = 0.001
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate,
    decay_steps=10000,  # You might need to tune this based on your data and batch size
    decay_rate=0.9,     # You might need to tune this
    staircase=True)


# Compile the model with optimizer, loss function, and metrics
model.compile(
optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']  #, tf.keras.metrics.TopKCategoricalAccuracy(k=5)
)

# Print summary sp far
print(model.summary())

# Load your data (keep as is from your original code)
df = load_data('3000sample_plain.csv')
df, cve_to_idx, idx_to_cve = preprocess_vulnerabilities(df)
vocab_size = len(cve_to_idx)

print(f"Loaded data with {len(df)} machines")
print(f"Vocabulary size: {vocab_size}")

# Create training data with a larger max_set_size initially
X_train, y_train = create_training_data(df, cve_to_idx, max_set_size=100)
X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42
)

print(f"Training examples: {len(X_train_split)}")
print(f"Validation examples: {len(X_val_split)}")

def create_optimized_model(vocab_size, embed_dim, num_heads, ff_dim,
                          num_transformer_blocks, max_set_size, learning_rate):
    """
    Modified model creation function that accepts hyperparameters as arguments
    """
    inputs = tf.keras.layers.Input(shape=(max_set_size,))

    embedding_layer = tf.keras.layers.Embedding(vocab_size + 1, embed_dim)
    x = embedding_layer(inputs)

    mask_layer = MaskingLayer()
    mask = mask_layer(inputs)
    x = x * mask

    # Stack transformer blocks
    for block_idx in range(num_transformer_blocks):
        transformer_block = SetAttentionBlock(embed_dim, num_heads, ff_dim)
        x = transformer_block(x)
        x = x * mask

    # Pooling
    set_lengths = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))(mask)
    x = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))(x)
    x = tf.keras.layers.Lambda(lambda x: x[0] / (x[1] + tf.keras.backend.epsilon()))([x, set_lengths])

    # Dense layers
    x = tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)
    x = tf.keras.layers.Dense(ff_dim, activation='relu')(x)
    x = tf.keras.layers.Dropout(0.2)(x)
    x = tf.keras.layers.Dense(ff_dim // 2, activation='relu')(x)
    x = tf.keras.layers.Dropout(0.2)(x)

    outputs = tf.keras.layers.Dense(vocab_size, activation='softmax')(x)

    model = tf.keras.Model(inputs, outputs)

    # Compile with exponential decay
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        learning_rate,
        decay_steps=10000,
        decay_rate=0.9,
        staircase=True
    )

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

print("Model creation function ready!")

def objective(trial):
    """
    Optuna objective function to optimize hyperparameters
    """

    # Define hyperparameter search spaces with justifications

    # embed_dim: Powers of 2 are computationally efficient for attention mechanisms
    embed_dim = trial.suggest_categorical('embed_dim', [32, 64, 128, 256])

    # num_heads: Must divide embed_dim evenly for multi-head attention
    possible_heads = [h for h in [2, 4, 8, 16] if embed_dim % h == 0]
    num_heads = trial.suggest_categorical('num_heads', possible_heads)

    # ff_dim: Typically 2-4x the embed_dim in transformers
    ff_dim_multiplier = trial.suggest_categorical('ff_dim_multiplier', [2, 4, 6, 8])
    ff_dim = embed_dim * ff_dim_multiplier

    # num_transformer_blocks: More blocks = more capacity but also more overfitting risk
    num_transformer_blocks = trial.suggest_int('num_transformer_blocks', 2, 6)

    # max_set_size: Should accommodate most of your vulnerability sets
    max_set_size = trial.suggest_categorical('max_set_size', [30, 50, 75, 100])

    # learning_rate: Log scale is standard for learning rates
    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)

    # batch_size: Larger batches = more stable gradients, smaller = more updates
    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])

    try:
        # Clear any existing models to free memory
        tf.keras.backend.clear_session()
        gc.collect()

        # Pad/truncate training data to match current max_set_size
        X_train_padded = []
        X_val_padded = []

        for x in X_train_split:
            if len(x) > max_set_size:
                x_padded = x[:max_set_size]
            else:
                x_padded = np.pad(x, (0, max_set_size - len(x)), 'constant')
            X_train_padded.append(x_padded)

        for x in X_val_split:
            if len(x) > max_set_size:
                x_padded = x[:max_set_size]
            else:
                x_padded = np.pad(x, (0, max_set_size - len(x)), 'constant')
            X_val_padded.append(x_padded)

        X_train_padded = np.array(X_train_padded)
        X_val_padded = np.array(X_val_padded)

        # Create model with trial hyperparameters
        model = create_optimized_model(
            vocab_size=vocab_size,
            embed_dim=embed_dim,
            num_heads=num_heads,
            ff_dim=ff_dim,
            num_transformer_blocks=num_transformer_blocks,
            max_set_size=max_set_size,
            learning_rate=learning_rate
        )

        # Train model with early stopping (shorter epochs for faster optimization)
        history = model.fit(
            X_train_padded,
            y_train_split,
            batch_size=batch_size,
            epochs=15,  # Reduced for faster optimization
            validation_data=(X_val_padded, y_val_split),
            callbacks=[
                tf.keras.callbacks.EarlyStopping(
                    patience=5,
                    restore_best_weights=True,
                    monitor='val_loss'
                )
            ],
            verbose=0  # Reduce output during optimization
        )

        # Get best validation accuracy
        best_val_accuracy = max(history.history['val_accuracy'])

        # Clean up
        del model
        tf.keras.backend.clear_session()
        gc.collect()

        return best_val_accuracy

    except Exception as e:
        print(f"Trial failed with error: {e}")
        return 0.0

print("Objective function ready!")

def run_optimization(n_trials=20):
    """
    Run Optuna optimization study

    Args:
        n_trials: Number of trials to run (start with 20-50 for testing)
    """

    # Create study object
    study = optuna.create_study(
        direction='maximize',  # We want to maximize validation accuracy
        sampler=optuna.samplers.TPESampler(seed=42),  # Tree-structured Parzen Estimator
        pruner=optuna.pruners.MedianPruner(  # Prune unpromising trials early
            n_startup_trials=5,
            n_warmup_steps=5
        )
    )

    # Run optimization
    study.optimize(objective, n_trials=n_trials)

    # Print results
    print("=" * 50)
    print("OPTIMIZATION COMPLETE!")
    print("=" * 50)
    print(f"Best trial: {study.best_trial.number}")
    print(f"Best validation accuracy: {study.best_value:.4f}")
    print("\nBest hyperparameters:")
    for key, value in study.best_params.items():
        print(f"  {key}: {value}")

    # Show parameter importance
    if len(study.trials) > 5:
        try:
            importance = optuna.importance.get_param_importances(study)
            print("\nParameter importance:")
            for param, imp in sorted(importance.items(), key=lambda x: x[1], reverse=True):
                print(f"  {param}: {imp:.3f}")
        except:
            print("Could not calculate parameter importance")

    return study

print("Optimization runner function ready!")

# Start the optimization process
print("Starting hyperparameter optimization...")
print(f"This will test different combinations of:")
print("- embed_dim: [32, 64, 128, 256]")
print("- num_heads: [2, 4, 8, 16] (depends on embed_dim)")
print("- ff_dim_multiplier: [2, 4, 6, 8]")
print("- num_transformer_blocks: [2, 3, 4, 5, 6]")
print("- max_set_size: [30, 50, 75, 100]")
print("- learning_rate: [0.0001, 0.01] (log scale)")
print("- batch_size: [16, 32, 64]")
print("\n" + "="*50)

# Run optimization - START WITH SMALL NUMBER FOR TESTING
study = run_optimization(n_trials=30)  # Increase this for more thorough search

def train_final_model(study):
    """
    Train the final model with the best hyperparameters found
    """
    best_params = study.best_params

    print("\n" + "="*50)
    print("TRAINING FINAL MODEL WITH BEST PARAMETERS")
    print("="*50)

    # Display best parameters
    print("Using these optimized hyperparameters:")
    for key, value in best_params.items():
        print(f"  {key}: {value}")

    # Prepare data with best max_set_size
    max_set_size = best_params['max_set_size']
    X_train_final, y_train_final = create_training_data(df, cve_to_idx, max_set_size=max_set_size)
    X_train_split_final, X_val_split_final, y_train_split_final, y_val_split_final = train_test_split(
        X_train_final, y_train_final, test_size=0.2, random_state=42
    )

    print(f"\nFinal training set: {len(X_train_split_final)} examples")
    print(f"Final validation set: {len(X_val_split_final)} examples")

    # Create final model
    final_model = create_optimized_model(
        vocab_size=vocab_size,
        embed_dim=best_params['embed_dim'],
        num_heads=best_params['num_heads'],
        ff_dim=best_params['embed_dim'] * best_params['ff_dim_multiplier'],
        num_transformer_blocks=best_params['num_transformer_blocks'],
        max_set_size=best_params['max_set_size'],
        learning_rate=best_params['learning_rate']
    )

    print("\nModel architecture:")
    print(final_model.summary())

    # Train with more epochs for final model
    print("\nTraining final model...")
    history = final_model.fit(
        X_train_split_final,
        y_train_split_final,
        batch_size=best_params['batch_size'],
        epochs=50,  # More epochs for final training
        validation_data=(X_val_split_final, y_val_split_final),
        callbacks=[
            tf.keras.callbacks.EarlyStopping(
                patience=10,
                restore_best_weights=True,
                monitor='val_loss'
            )
        ],
        verbose=1
    )

    # Evaluate final model
    print("\nEvaluating final model...")
    y_pred = final_model.predict(X_val_split_final).argmax(axis=1)
    print("\nFinal model classification report:")
    print(classification_report(y_val_split_final, y_pred))

    return final_model, history

# Train the final model with optimized hyperparameters
final_model, final_history = train_final_model(study)

print("\n" + "="*60)
print("OPTIMIZATION SUMMARY")
print("="*60)

print(f"âœ… Completed {len(study.trials)} trials")
print(f"âœ… Best validation accuracy: {study.best_value:.4f}")
print(f"âœ… Best trial was #{study.best_trial.number}")

print("\nYour original hyperparameters:")
print("  embed_dim: 64")
print("  num_heads: 4")
print("  ff_dim: 512")
print("  num_transformer_blocks: 4")
print("  max_set_size: 50")

print(f"\nOptimized hyperparameters:")
for key, value in study.best_params.items():
    print(f"  {key}: {value}")

if study.best_params['embed_dim'] != 64:
    print(f"\nðŸ’¡ Embed dimension changed: 64 â†’ {study.best_params['embed_dim']}")

if study.best_params['num_heads'] != 4:
    print(f"ðŸ’¡ Number of heads changed: 4 â†’ {study.best_params['num_heads']}")

original_ff_dim = 512
optimized_ff_dim = study.best_params['embed_dim'] * study.best_params['ff_dim_multiplier']
if optimized_ff_dim != original_ff_dim:
    print(f"ðŸ’¡ FF dimension changed: 512 â†’ {optimized_ff_dim}")

if study.best_params['num_transformer_blocks'] != 4:
    print(f"ðŸ’¡ Transformer blocks changed: 4 â†’ {study.best_params['num_transformer_blocks']}")

if study.best_params['max_set_size'] != 50:
    print(f"ðŸ’¡ Max set size changed: 50 â†’ {study.best_params['max_set_size']}")

print(f"\nðŸŽ¯ Final model is ready for use!")
print(f"ðŸ”§ You can now use these optimized hyperparameters in your future experiments.")

print("\n" + "="*60)