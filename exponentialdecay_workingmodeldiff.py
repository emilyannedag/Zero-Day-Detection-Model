# -*- coding: utf-8 -*-
"""ExponentialDecay_WorkingModelDiff.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/174nsoNSBwglhNPXBgQoYqsREaOawSlbU
"""

pip install tensorflow pandas openpyxl scikit-learn numpy matplotlib

import pandas as pd              # Data analysis and handling a .csv or .xlsx
import numpy as np              # For math and handling arrays
import tensorflow as tf         # deep learning framework
from tensorflow.keras import regularizers
from sklearn.preprocessing import LabelEncoder  # labelling categorical data (CVE)
from sklearn.model_selection import train_test_split  # for splitting the data
import random                   # needed for masking
from collections import Counter # needed for counting in this model

def load_data(file_path):

    # we want either a .csv or .xlsx, so we must check it first
    if file_path.endswith('.csv'):
        # use pandas to put the data from the .csv into a df
        # just incase any wonky data (probably not a problem? Maybe delete?), on_bad_lines='skip'
        df = pd.read_csv(file_path, engine='python')
    elif file_path.endswith('.xlsx'):
        # use pandas to put the data from the .xlsx into a df
        df = pd.read_excel(file_path)
    else:
        # error
        raise ValueError("Upload .csv or .xlsx only")

    # return loaded DataFrame
    return df

def preprocess_vulnerabilities(df, vulnerability_column='Vulnerabilities'):
    """

    This function takes the comma-separated strings of CVEs and:
    1. Converts them to lists
    2. Creates a list of all unique CVEs
    3. Creates bidirectional mappings between CVEs and numerical indices, by Google's advice

    Args:
        df: pandas DataFrame containing the data
        vulnerability_column: name of column containing vulnerability strings

    Returns:
        df: DataFrame with an added column -> 'vuln_list'
        cve_to_idx: dictionary mapping CVE strings to indices
        idx_to_cve: dictionary mapping indices to CVE strings
        ^^actually, unclear why we need the bidirectional mapping?
    """

    # Convert string of CVEs to lists for each row for formatting
    # lambda function processes each cell in the vulnerability column
    df['vuln_list'] = df[vulnerability_column].apply(
        lambda x: [
            cve.strip()  # Removes whitespace
            for cve in str(x).split(',')  # Split by comma to get individual CVEs
            if cve.strip()  # Only keep non-empty CVEs after stripping whitespace, although this shouldnt be a problem
        ]
    )

    # Compile all unique CVEs
    all_cves = []
    # Iterate through each machine's vulnerability list
    for vuln_list in df['vuln_list']:
        # Add all vulnerabilities from this machine to the master list
        all_cves.extend(vuln_list)

    # Get unique CVEs only (remove duplicates by converting to set, then back to list)
    unique_cves = list(set(all_cves))
    # Print what we have so far
    print(f"Total unique CVEs: {len(unique_cves)}")

    # Create mapping from CVE string to numerical index
    # needed because neural networks work with numbers, basically just formatting
    cve_to_idx = {cve: idx for idx, cve in enumerate(unique_cves)}

    # Create reverse mapping from numerical index back to CVE string
    # Google says its for interpreting model predictions, but I dont get this one
    idx_to_cve = {idx: cve for cve, idx in cve_to_idx.items()}

    # Return processed DataFrame and both mappings
    return df, cve_to_idx, idx_to_cve

# the actual file (**Change this when uploading different file)
df = load_data('Min3Vulns_Plain_Additional.csv')

# recursive call with my actual file
df, cve_to_idx, idx_to_cve = preprocess_vulnerabilities(df)

# store # of unique CVEs as vocab_size for model architecture
vocab_size = len(cve_to_idx)

def create_training_data(df, cve_to_idx, max_set_size=75):
    """
    Create training data where one CVE is 'masked' and needs to be predicted

    training strategy: teach the model to predict missing vulnerabilities by showing it incomplete sets and asking it to guess what's missing.

    Args:
        df: pandas DataFrame with vulnerability data
        cve_to_idx: mapping from CVE strings to indices
        max_set_size: maximum number of CVEs per training example (for padding)

    Returns:
        X_input: numpy array of input vulnerability sets (with one CVE masked)
        y_target: numpy array of target CVEs that were masked
    """

    # Initialize lists to store training examples
    X_input = []   # stores input sets (vulnerability lists with 1 CVE removed)
    y_target = []  # stores the target CVE that was removed (what we want predicted)


    for _, row in df.iterrows():
        # Get the list of vulnerabilities for each row
        vuln_list = row['vuln_list']

        # Skip machines with less than 2, **maybe delete this? dont need it anymore
        if len(vuln_list) < 2:
            continue

        # creating up to 3 examples per machine
        for _ in range(min(3, len(vuln_list))):
            # Double-check the # of the vulns (maybe delete?)
            if len(vuln_list) >= 2:
                # Randomly select 1 CVE to mask
                masked_idx = random.randint(0, len(vuln_list) - 1)
                # Hidden one "target"
                target_cve = vuln_list[masked_idx]

                # Create input set (all CVEs except the masked one)
                input_cves = vuln_list[:masked_idx] + vuln_list[masked_idx+1:]

                # Convert CVE strings to indices bc models need numbers, not strings
                # check (cve in cve_to_idx)
                input_indices = [
                    cve_to_idx[cve]
                    for cve in input_cves
                    if cve in cve_to_idx  # Only CVEs that exist in the list
                ]

                # Convert target CVE also to index, -1 if not found
                target_idx = cve_to_idx.get(target_cve, -1)

                # add this example if both target and input are valid (not -1 or something)
                if target_idx != -1 and len(input_indices) > 0:
                    # Handle sequence length: pad or truncate to max_set_size
                    if len(input_indices) > max_set_size:
                        # If too many vulnerabilities, keep only the first max_set_size
                        input_indices = input_indices[:max_set_size]
                    else:
                        # If too few vulnerabilities, pad with zeros
                        padding_needed = max_set_size - len(input_indices)
                        input_indices.extend([0] * padding_needed)

                    # add training example to lists
                    X_input.append(input_indices)
                    y_target.append(target_idx)

    # Convert lists to numpy arrays (required for TensorFlow)
    return np.array(X_input), np.array(y_target)

# Create the training data using masking (recursive call)
X_train, y_train = create_training_data(df, cve_to_idx)
# Print stats about training data
print(f"Training examples: {len(X_train)}")

class MultiHeadAttention(tf.keras.layers.Layer):
    """
    Basically allows the model to focus on different parts of the input set at the same time
    """

    def __init__(self, embed_dim, num_heads):
        """
        Initialize Multi-Head Attention layer

        Args:
            embed_dim: dimensionality of the embedding space
            num_heads: number of attention heads to use

        Admittedly, I got most of this from keras documentation/guides
        used to learn set-based representations of CVEs
        """
        # Call parent class
        super(MultiHeadAttention, self).__init__()

        # from documentation (shrug)
        self.embed_dim = embed_dim
        self.num_heads = num_heads

        # from documentation (shrug)
        assert embed_dim % num_heads == 0

        # from documentation (shrug)
        self.projection_dim = embed_dim // num_heads

        # from documentation (shrug)
        self.query_dense = tf.keras.layers.Dense(embed_dim)
        self.key_dense = tf.keras.layers.Dense(embed_dim)
        self.value_dense = tf.keras.layers.Dense(embed_dim)

        # from documentation (shrug)
        self.combine_heads = tf.keras.layers.Dense(embed_dim)

    def attention(self, query, key, value):
        """
        Args:
            query: what we're looking for
            key: what we're looking in
            value: what we actually return

        Returns:
            output: attention-weighted values
            weights: attention weights (for interpretability)
            All basically from documentation/guides VVV
        """

        score = tf.matmul(query, key, transpose_b=True)

        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)

        scaled_score = score / tf.math.sqrt(dim_key)

        weights = tf.nn.softmax(scaled_score, axis=-1)

        output = tf.matmul(weights, value)

        return output, weights

    def separate_heads(self, x, batch_size):
        """
 From documentation
        Args:
            x: input tensor
            batch_size: size of the current batch

        Returns:
            reshaped tensor with separate heads
        """
        # Reshaping for tf
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))

        # Transposing for tf
        # grouping all heads together bc efficient I think
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, inputs):
        """
        Forward pass of the Multi-Head Attention layer

        Args:
            inputs: input tensor

        Returns:
            output tensor after applying multi-head attention
        """
        # Get batch size from input shape
        batch_size = tf.shape(inputs)[0]

        # Transform inputs to queries, keys, and values using learned linear transformations
        query = self.query_dense(inputs)
        key = self.key_dense(inputs)
        value = self.value_dense(inputs)

        # Separate each into multiple heads for parallel attention
        query = self.separate_heads(query, batch_size)
        key = self.separate_heads(key, batch_size)
        value = self.separate_heads(value, batch_size)

        # Apply attention mechanism
        attention, weights = self.attention(query, key, value)

        # Transpose back: (batch_size, num_heads, seq_length, projection_dim) -> (batch_size, seq_length, num_heads, projection_dim)
        attention = tf.transpose(attention, perm=[0, 2, 1, 3])

        # Concatenate all heads: (batch_size, seq_length, num_heads, projection_dim) -> (batch_size, seq_length, embed_dim)
        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))

        # Apply final linear transformation to combine information from all heads
        output = self.combine_heads(concat_attention)

        return output

class SetAttentionBlock(tf.keras.layers.Layer):
    """
    A complete Transformer block combining Multi-Head Attention with Feed-Forward Network

    This is one "layer" of the Transformer. It includes:
    1. Multi-head attention (to find relationships between vulnerabilities)
    2. Feed-forward network (to process the attended information)
    3. Residual connections (to help with training deep networks)
    4. Layer normalization (to stabilize training)
    """

    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        """
        More from docs

        Args:
            embed_dim: embedding dimension
            num_heads: number of attention heads
            ff_dim: dimension of feed-forward network
            rate: dropout rate for regularization
        """
        super(SetAttentionBlock, self).__init__()


        self.att = MultiHeadAttention(embed_dim, num_heads)


        self.ffn = tf.keras.Sequential([
            tf.keras.layers.Dense(ff_dim, activation="relu"),
            tf.keras.layers.Dense(embed_dim),
        ])


        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

        #prevent overfitting
        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)

    def call(self, inputs, training=None):
        """
        More from docs

        Args:
            inputs: input tensor
            training: whether we're in training mode (affects dropout)
                     If None, Keras will automatically determine based on context

        Returns:
            output tensor after applying attention and feed-forward processing
        """

        attn_output = self.att(inputs)


        attn_output = self.dropout1(attn_output, training=training)


        out1 = self.layernorm1(inputs + attn_output)


        ffn_output = self.ffn(out1)


        ffn_output = self.dropout2(ffn_output, training=training)


        return self.layernorm2(out1 + ffn_output)

class MaskingLayer(tf.keras.layers.Layer):
    def call(self, inputs):
        mask = tf.cast(tf.not_equal(inputs, 0), tf.float32)
        mask = tf.expand_dims(mask, axis=-1)
        return mask
#maybe mess with max set size, as in, I believe some machines has 100+ vulns
#also worth experimenting with embed_dim=128, num_heads=8, ff_dim=512, embed_dim=64, num_heads=4 for example
def create_set_transformer_model(vocab_size, embed_dim=256, num_heads=4, ff_dim=1536,
                                num_transformer_blocks=2, max_set_size=75):
    """
    Actually creating Set Transformer model for vulnerability co-occurrence prediction

    building with:
    1. Embedding layer (converts CVE indices to dense vectors)
    2. Multiple Set Attention Blocks (the core Transformer processing)
    3. Pooling layer (combines variable-length sets into fixed-size vectors)
    4. Classification layers (predicts which CVE was masked)

    Args:
        vocab_size: number of unique CVEs in vocabulary
        embed_dim: dimension of embedding vectors
        num_heads: number of attention heads per block
        ff_dim: dimension of feed-forward networks
        num_transformer_blocks: number of transformer layers to stack
        max_set_size: maximum number of CVEs per input set

    Returns:
        compiled Keras model ready for training
    """

    # Define input layer: expects sequences of integers (CVE indices)
    # Shape: (batch_size, max_set_size)
    inputs = tf.keras.layers.Input(shape=(max_set_size,))

    # Embedding layer: converts integer indices to dense vectors
    # vocab_size + 1 bc index 0 is for padding
    # Each CVE gets mapped to a dense vector of size embed_dim
    embedding_layer = tf.keras.layers.Embedding(vocab_size + 1, embed_dim)
    x = embedding_layer(inputs)
    # x shape: (batch_size, max_set_size, embed_dim)

    # Create mask using MaskingLayer
    mask_layer = MaskingLayer()
    mask = mask_layer(inputs)
    # mask shape: (batch_size, max_set_size, 1)

    # Apply mask to embeddings (also set padding positions to zero)
    x = x * mask

    # each block does more and more pattern learning
    for block_idx in range(num_transformer_blocks):

        transformer_block = SetAttentionBlock(embed_dim, num_heads, ff_dim)
        x = transformer_block(x)
        x = x * mask #going to re-apply the mask to make sure padding = zero
#a lot from docs again here
    set_lengths = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))(mask)


    x = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))(x)

    x = tf.keras.layers.Lambda(lambda x: x[0] / (x[1] + tf.keras.backend.epsilon()))([x, set_lengths])




    # First hidden layer with ReLU activation
    x = tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)


    x = tf.keras.layers.Dense(ff_dim, activation='relu')(x)

    # Add dropout for regularization
    x = tf.keras.layers.Dropout(0.2)(x) # test out 0.3–0.5

    # Second hidden layer (smaller)
    x = tf.keras.layers.Dense(ff_dim // 2, activation='relu')(x)
    # Add more dropout
    x = tf.keras.layers.Dropout(0.2)(x)

    # Output layer: predict probability distribution over all possible CVEs
    # vocab_size outputs (one for each possible CVE)
    # Softmax activation ensures outputs sum to 1 (valid probability distribution)
    outputs = tf.keras.layers.Dense(vocab_size, activation='softmax')(x)

    # Create the complete model
    model = tf.keras.Model(inputs, outputs)

    return model

# Create the model with vocab_size
model = create_set_transformer_model(vocab_size)


# SOME NEW STUFF FOR EXPONENTIAL DECAY ****************
initial_learning_rate = 0.00010980988314820266 #changed from .0001
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate,
    decay_steps=10000,  # You might need to tune this based on your data and batch size
    decay_rate=0.9,     # You might need to tune this
    staircase=True)


# Compile the model with optimizer, loss function, and metrics
model.compile(
optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']  #, tf.keras.metrics.TopKCategoricalAccuracy(k=5)
)

# Print summary sp far
print(model.summary())

"""Now that the model is defined and compiled, we can train it using the prepared training and validation data."""

# Split training data into seperate training and validation sets
X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
    X_train,     # vulnerability sets with one CVE masked
    y_train,     # the masked CVE indices
    test_size=0.2,    # Use 20% of data for validation
    random_state=42   # popular fixed seed number
)

# Print dataset stats so far
print(f"Training set: {len(X_train_split)} examples")
print(f"Validation set: {len(X_val_split)} examples")

# Training the model with only exponential decay (no conflicting callbacks)
history = model.fit(
    X_train_split,           # Training input data
    y_train_split,           # Training target/validation labels
    batch_size=16,           # 32 examples at a time (google said this was good)
    epochs=60,               # 50 -> 60 # of epochs
    validation_data=(X_val_split, y_val_split),  # Data for validation during training
    callbacks=[
        # stop training if validation loss doesn't improve for 10 epochs
        tf.keras.callbacks.EarlyStopping(
            patience=10,           # Wait 10 epochs before stopping
            restore_best_weights=True  # Restore weights from best epoch
        )
        # Removed ReduceLROnPlateau to avoid conflicting with ExponentialDecay
    ]
)

# Evaluation with classification report
from sklearn.metrics import classification_report

# Predict top-1 on validation
y_pred = model.predict(X_val_split).argmax(axis=1)
print(classification_report(y_val_split, y_pred))
